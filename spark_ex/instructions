# spark submit kafka local
spark-submit --jars $(echo /Users/tranlammacbook/Documents/spark_streaming_kafka/spark_ex/target/dependency/*.jar | tr ' ' ',') --class com.tranlam.App target/spark_ex-1.0-SNAPSHOT.jar --app-name ingest_avro_from_kafka_local --jdbc-url "jdbc:postgresql://localhost:5432/postgres?user=postgres&password=postgres"

# spark submit kafka spark cluster
$SPARK_HOME/bin/spark-submit --jars $(echo /execution_files/dependency/*.jar | tr ' ' ',') --class com.tranlam.App /execution_files/spark_ex-1.0-SNAPSHOT.jar --app-name ingest_avro_from_kafka --jdbc-url "jdbc:postgresql://db:5432/postgres?user=postgres&password=postgres"

# spark submit import with correct unicode character set
/bin/spark-submit \
  --name "db_to_hdfs_snapshot" \
  --deploy-mode cluster \
  --executor-cores 4 \
  --driver-memory 1G \
  --executor-memory 5G \
  --num-executors 1 \
  --queue sync_jobs \
  --conf spark.cleaner.periodicGC.interval=2min \
  --conf spark.driver.maxResultSize=2g \
  --conf spark.debug.maxToStringFields=5000 \
  --conf spark.sql.debug.maxToStringFields=5000 \
  --conf spark.driver.memoryOverhead=1536 \
  --conf spark.executor.memoryOverhead=4088 \
  --conf spark.dynamicAllocation.enabled=false \
  --conf spark.dynamicAllocation.minExecutors=1 \
  --conf spark.streaming.concurrentjobs=1 \
  --conf spark.kryoserializer.buffer.max=512m \
  --conf spark.dynamicAllocation.maxExecutors=4 \
  --conf spark.dynamicAllocation.initialExecutors=1 \
  --conf spark.shuffle.compress=true \
  --jars hdfs://nameservice1/user/etl/spark_jobs/ssk/libs/* \
  --class com.tranlam.DBtoHDFS \
  /home/sync_etl/lamtt50/db_to_hdfs/spark_ex-1.0-SNAPSHOT.jar \
  --app-master yarn \
  --jdbc-url 'jdbc:mysql://10.110.97.180:3306/qc_tool?user=etl&password=nVTwZWcvuzUUh9aE^*^&zeroDateTimeBehavior=CONVERT_TO_NULL' \
  --db-table "(SELECT * FROM qc_tool.cs_task_states WHERE created_at >= '2022-06-24' AND created_at  < '2022-06-29') AS mytable" \
  --hdfs-path '/user/sync_etl/transfer/ingest/qc_tool/cs_task_states' \
  --driver-class 'com.mysql.jdbc.Driver' \
  --query-stm "SELECT * FROM tempViewForSnapshot LIMIT 10" \
  --num-partitions 2

  --query-stm 'SELECT id, task_id, state_id, user_id, data, created_at, updated_at FROM tempViewForSnapshot'

# run multiple lines impala queries
read -r -d '' CMD << EOM
create table test.category_test
stored as parquet
as
select id,
name,
status,
created_at,
updated_at,
created_by,
description,
parent_id
from test.category;
EOM

/usr/java/jdk1.8.0_181-cloudera/bin/java -cp /home/sync_etl/lamtt50/execute_impala_query/spark_ex-1.0-SNAPSHOT.jar:/home/sync_etl/jobs/ghtk-ingestion-tool-dev/libs/* utils.ImpalaUtils com.cloudera.impala.jdbc41.Driver 'jdbc:impala://impala-proxy:6050/;AuthMech=1;KrbRealm=ghtk.vn;KrbHostFQDN=impala-proxy;KrbServiceName=impala' "$CMD"

